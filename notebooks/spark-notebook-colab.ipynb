{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "title-header",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Guía Práctica de Apache Spark para Big Data\n",
    "\n",
    "Este notebook proporciona una introducción práctica a Apache Spark, desde conceptos básicos hasta operaciones intermedias, utilizando PySpark en Google Colab.\n",
    "\n",
    "## Contenido\n",
    "1. Configuración del entorno en Colab\n",
    "2. Introducción a Apache Spark\n",
    "3. Creación y Operaciones con RDDs\n",
    "4. DataFrames y Datasets\n",
    "5. Spark SQL\n",
    "6. Ejercicio práctico: Análisis de datos reales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1. Configuración del entorno en Colab\n",
    "\n",
    "Primero, necesitamos instalar y configurar PySpark en Google Colab. Ejecuta la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install-spark"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Spark está configurado y listo para usar!\n"
     ]
    }
   ],
   "source": [
    "# Reemplaza la sección de instalación con:\n",
    "import findspark\n",
    "findspark.init()\n",
    "print(\"¡Spark está configurado y listo para usar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-session"
   },
   "source": [
    "### Crear una SparkSession\n",
    "\n",
    "La SparkSession es el punto de entrada para interactuar con Spark. A partir de Spark 2.0, la SparkSession proporciona un punto de entrada unificado a todas las funcionalidades de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "spark-session"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Apache Spark: 3.5.0\n",
      "URL de la interfaz web: http://eeefdd88d7c2:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Tutorial\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verificar versión de Spark\n",
    "print(f\"Versión de Apache Spark: {spark.version}\")\n",
    "\n",
    "# SparkContext está disponible como sc\n",
    "sc = spark.sparkContext\n",
    "print(f\"URL de la interfaz web: {sc.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro-spark"
   },
   "source": [
    "## 2. Introducción a Apache Spark\n",
    "\n",
    "Apache Spark es un framework de computación distribuida diseñado para el procesamiento de grandes volúmenes de datos. Lo que distingue a Spark de otros frameworks como Hadoop MapReduce es su capacidad para procesar datos en memoria, lo que lo hace significativamente más rápido.\n",
    "\n",
    "### Características principales de Spark\n",
    "- **Velocidad**: 100x más rápido que Hadoop MapReduce para procesamiento en memoria\n",
    "- **Facilidad de uso**: APIs en Java, Scala, Python y R\n",
    "- **Generalidad**: Combina SQL, streaming y análisis complejo\n",
    "- **Compatibilidad**: Funciona con diversas fuentes de datos (HDFS, S3, HBase, etc.)\n",
    "\n",
    "### Arquitectura de Spark\n",
    "- **Driver Program**: Contiene la aplicación principal y crea el SparkContext\n",
    "- **Cluster Manager**: Asigna recursos (YARN, Mesos, Kubernetes, Standalone)\n",
    "- **Worker Nodes**: Ejecutan las tareas de computación\n",
    "- **Executors**: Procesos JVM que ejecutan tareas en cada nodo\n",
    "\n",
    "### Componentes del Ecosistema Spark\n",
    "- **Spark Core**: Base del sistema, APIs para RDDs\n",
    "- **Spark SQL**: Procesamiento de datos estructurados\n",
    "- **Spark Streaming**: Procesamiento en tiempo real\n",
    "- **MLlib**: Biblioteca de machine learning\n",
    "- **GraphX**: Procesamiento de grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdds-section"
   },
   "source": [
    "## 3. Creación y Operaciones con RDDs\n",
    "\n",
    "Los RDDs (Resilient Distributed Datasets) son la abstracción fundamental de Spark. Son colecciones inmutables de objetos distribuidos a través de un clúster, que pueden ser procesados en paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-rdds"
   },
   "source": [
    "### Creación de RDDs\n",
    "\n",
    "Hay dos formas principales de crear RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "parallelize"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD1: ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n",
      "Número de particiones: 4\n",
      "Primeros 3 elementos: [1, 2, 3]\n",
      "Conteo: 10\n"
     ]
    }
   ],
   "source": [
    "# 1. Paralelizando una colección existente\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd1 = sc.parallelize(data, numSlices=4)  # numSlices es el número de particiones\n",
    "\n",
    "print(f\"RDD1: {rdd1}\")\n",
    "print(f\"Número de particiones: {rdd1.getNumPartitions()}\")\n",
    "print(f\"Primeros 3 elementos: {rdd1.take(3)}\")\n",
    "print(f\"Conteo: {rdd1.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "textfile-rdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD2: ejemplo.txt MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0\n",
      "Contenido: ['Línea 1\\\\nLínea 2\\\\nLínea 3\\\\nLínea 4\\\\nLínea 5']\n"
     ]
    }
   ],
   "source": [
    "# 2. Referenciando un dataset externo\n",
    "# Primero creamos un archivo de ejemplo\n",
    "!echo \"Línea 1\\nLínea 2\\nLínea 3\\nLínea 4\\nLínea 5\" > ejemplo.txt\n",
    "\n",
    "# Luego creamos un RDD a partir del archivo\n",
    "rdd2 = sc.textFile(\"ejemplo.txt\")\n",
    "\n",
    "print(f\"RDD2: {rdd2}\")\n",
    "print(f\"Contenido: {rdd2.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdd-operations"
   },
   "source": [
    "### Operaciones con RDDs\n",
    "\n",
    "Las operaciones en RDDs se dividen en dos categorías:\n",
    "\n",
    "- **Transformaciones**: Crean un nuevo RDD a partir de uno existente (map, filter, etc.)\n",
    "- **Acciones**: Devuelven un valor al driver después de ejecutar un cálculo (reduce, count, etc.)\n",
    "\n",
    "Las transformaciones son perezosas (lazy), lo que significa que no se ejecutan inmediatamente. En cambio, Spark recuerda las transformaciones aplicadas a un RDD y las ejecuta solo cuando se requiere una acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "transformations"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de map (x²): [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "Números pares: [2, 4, 6, 8, 10]\n",
      "Palabras individuales: ['Hola', 'mundo', 'Apache', 'Spark', 'Big', 'Data']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de transformaciones\n",
    "\n",
    "# map: aplica una función a cada elemento del RDD\n",
    "squared = rdd1.map(lambda x: x * x)\n",
    "print(f\"Después de map (x²): {squared.collect()}\")\n",
    "\n",
    "# filter: selecciona elementos que cumplen una condición\n",
    "evens = rdd1.filter(lambda x: x % 2 == 0)\n",
    "print(f\"Números pares: {evens.collect()}\")\n",
    "\n",
    "# flatMap: aplica función que devuelve múltiples elementos\n",
    "rdd_text = sc.parallelize([\"Hola mundo\", \"Apache Spark\", \"Big Data\"])\n",
    "words = rdd_text.flatMap(lambda line: line.split(\" \"))\n",
    "print(f\"Palabras individuales: {words.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "actions"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma de todos los elementos: 55\n",
      "Número de elementos: 10\n",
      "Primer elemento: 1\n",
      "Primeros 3 elementos: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de acciones\n",
    "\n",
    "# reduce: agrega los elementos usando una función\n",
    "sum_all = rdd1.reduce(lambda a, b: a + b)\n",
    "print(f\"Suma de todos los elementos: {sum_all}\")\n",
    "\n",
    "# count: devuelve el número de elementos\n",
    "count = rdd1.count()\n",
    "print(f\"Número de elementos: {count}\")\n",
    "\n",
    "# first: devuelve el primer elemento\n",
    "first_element = rdd1.first()\n",
    "print(f\"Primer elemento: {first_element}\")\n",
    "\n",
    "# take: devuelve n elementos\n",
    "first_n = rdd1.take(3)\n",
    "print(f\"Primeros 3 elementos: {first_n}\")\n",
    "\n",
    "# foreach: ejecuta una función en cada elemento (sin retorno)\n",
    "rdd1.foreach(lambda x: print(f\"Elemento: {x}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pair-rdds"
   },
   "source": [
    "### RDDs de pares (key-value)\n",
    "\n",
    "Los RDDs de pares son RDDs con elementos en forma de tuplas (clave, valor). Estos RDDs ofrecen operaciones adicionales como reduceByKey, join, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pair-operations"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD de pares: [('a', 1), ('b', 2), ('a', 3), ('c', 4), ('b', 5), ('c', 6)]\n",
      "Suma por clave: [('c', 10), ('a', 4), ('b', 7)]\n",
      "Agrupado por clave: [('c', [4, 6]), ('a', [1, 3]), ('b', [2, 5])]\n",
      "Join de RDDs: [('c', (4, 'z')), ('c', (6, 'z')), ('a', (1, 'x')), ('a', (3, 'x')), ('b', (2, 'y')), ('b', (5, 'y'))]\n",
      "Conteo por clave: defaultdict(<class 'int'>, {'a': 2, 'b': 2, 'c': 2})\n"
     ]
    }
   ],
   "source": [
    "# Crear un RDD de pares\n",
    "pairs_rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"c\", 4), (\"b\", 5), (\"c\", 6)])\n",
    "print(f\"RDD de pares: {pairs_rdd.collect()}\")\n",
    "\n",
    "# reduceByKey: combina valores con la misma clave\n",
    "sums = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(f\"Suma por clave: {sums.collect()}\")\n",
    "\n",
    "# groupByKey: agrupa valores con la misma clave\n",
    "grouped = pairs_rdd.groupByKey().mapValues(list)\n",
    "print(f\"Agrupado por clave: {grouped.collect()}\")\n",
    "\n",
    "# Crear otro RDD de pares para demostraciones\n",
    "other_rdd = sc.parallelize([(\"a\", \"x\"), (\"b\", \"y\"), (\"c\", \"z\")])\n",
    "\n",
    "# join: une RDDs por clave\n",
    "joined = pairs_rdd.join(other_rdd)\n",
    "print(f\"Join de RDDs: {joined.collect()}\")\n",
    "\n",
    "# countByKey: cuenta ocurrencias de cada clave\n",
    "counts = pairs_rdd.countByKey()\n",
    "print(f\"Conteo por clave: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "persistence"
   },
   "source": [
    "### Persistencia (Caching)\n",
    "\n",
    "Cuando queremos reutilizar un RDD múltiples veces, podemos persistirlo en memoria o disco para mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cache-persist"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin persistencia - Primera ejecución: 0.26 segundos\n",
      "Sin persistencia - Segunda ejecución: 0.22 segundos\n",
      "Con persistencia - Primera ejecución: 0.26 segundos\n",
      "Con persistencia - Segunda ejecución: 0.12 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[40] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un RDD y realizar una operación costosa\n",
    "large_rdd = sc.parallelize(range(1, 1000000))\n",
    "filtered = large_rdd.filter(lambda x: x % 10 == 0)\n",
    "\n",
    "# Sin persistencia (calculará dos veces)\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "count1 = filtered.count()\n",
    "end1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "count2 = filtered.count()\n",
    "end2 = time.time() - start\n",
    "\n",
    "print(f\"Sin persistencia - Primera ejecución: {end1:.2f} segundos\")\n",
    "print(f\"Sin persistencia - Segunda ejecución: {end2:.2f} segundos\")\n",
    "\n",
    "# Con persistencia\n",
    "filtered.cache()  # equivalente a filtered.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "start = time.time()\n",
    "count3 = filtered.count()\n",
    "end3 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "count4 = filtered.count()\n",
    "end4 = time.time() - start\n",
    "\n",
    "print(f\"Con persistencia - Primera ejecución: {end3:.2f} segundos\")\n",
    "print(f\"Con persistencia - Segunda ejecución: {end4:.2f} segundos\")\n",
    "\n",
    "# Liberar RDD de memoria\n",
    "filtered.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "word-count-example"
   },
   "source": [
    "### Ejemplo clásico: WordCount\n",
    "\n",
    "Implementaremos el clásico ejemplo de contar palabras en un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wordcount"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: 5\n",
      "de: 3\n",
      "y: 3\n",
      "es: 2\n",
      "procesamiento: 2\n",
      "para: 2\n",
      "un: 1\n",
      "framework: 1\n",
      "distribuido: 1\n",
      "rápido: 1\n",
      "fácil: 1\n",
      "usar: 1\n",
      "puede: 1\n",
      "memoria: 1\n",
      "java: 1\n",
      "python: 1\n",
      "incluye: 1\n",
      "streaming: 1\n",
      "machine: 1\n",
      "learning: 1\n",
      "procesar: 1\n",
      "datos: 1\n",
      "en: 1\n",
      "tiene: 1\n",
      "apis: 1\n",
      "scala: 1\n",
      "r: 1\n",
      "módulos: 1\n",
      "sql: 1\n",
      "grafos: 1\n"
     ]
    }
   ],
   "source": [
    "# Crear un texto de ejemplo\n",
    "text = \"\"\"Spark es un framework de procesamiento distribuido.\n",
    "Spark es rápido y fácil de usar.\n",
    "Spark puede procesar datos en memoria.\n",
    "Spark tiene APIs para Java, Scala, Python y R.\n",
    "Spark incluye módulos para SQL, streaming, machine learning y procesamiento de grafos.\"\"\"\n",
    "\n",
    "# Crear archivo de texto\n",
    "with open('ejemplo_wordcount.txt', 'w') as f:\n",
    "    f.write(text)\n",
    "\n",
    "# WordCount en Spark\n",
    "lines = sc.textFile(\"ejemplo_wordcount.txt\")\n",
    "\n",
    "# Separar líneas en palabras y convertir a minúsculas\n",
    "words = lines.flatMap(lambda line: line.lower().split())\n",
    "\n",
    "# Eliminar signos de puntuación\n",
    "import re\n",
    "clean_words = words.map(lambda word: re.sub(r'[^\\w]', '', word))\n",
    "\n",
    "# Filtrar palabras vacías\n",
    "filtered_words = clean_words.filter(lambda word: word != '')\n",
    "\n",
    "# Crear pares (palabra, 1)\n",
    "word_pairs = filtered_words.map(lambda word: (word, 1))\n",
    "\n",
    "# Sumar ocurrencias de cada palabra\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Ordenar por frecuencia (de mayor a menor)\n",
    "sorted_counts = word_counts.sortBy(lambda x: -x[1])\n",
    "\n",
    "# Mostrar resultados\n",
    "for word, count in sorted_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataframes-section"
   },
   "source": [
    "## 4. DataFrames y Datasets\n",
    "\n",
    "A partir de Spark 1.3, se introdujo el concepto de DataFrames, una abstracción de más alto nivel que los RDDs. Los DataFrames representan datos estructurados en formato de tabla, similares a las tablas en bases de datos relacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create-dataframes"
   },
   "source": [
    "### Creación de DataFrames\n",
    "\n",
    "Hay varias formas de crear DataFrames en Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "df-from-data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame desde lista:\n",
      "+------+----+\n",
      "|nombre|edad|\n",
      "+------+----+\n",
      "|  Juan|  30|\n",
      "|   Ana|  25|\n",
      "|Carlos|  35|\n",
      "| María|  28|\n",
      "+------+----+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Desde una lista de datos\n",
    "data = [(\"Juan\", 30), (\"Ana\", 25), (\"Carlos\", 35), (\"María\", 28)]\n",
    "df1 = spark.createDataFrame(data, [\"nombre\", \"edad\"])\n",
    "\n",
    "print(\"DataFrame desde lista:\")\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "df-from-rdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame desde RDD:\n",
      "+------------+-----------+\n",
      "|departamento|presupuesto|\n",
      "+------------+-----------+\n",
      "|   Marketing|       1000|\n",
      "|      Ventas|       2000|\n",
      "|          IT|       1500|\n",
      "|          RH|        800|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Desde un RDD\n",
    "rdd = sc.parallelize([(\"Marketing\", 1000), (\"Ventas\", 2000), (\"IT\", 1500), (\"RH\", 800)])\n",
    "df2 = rdd.toDF([\"departamento\", \"presupuesto\"])\n",
    "\n",
    "print(\"DataFrame desde RDD:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "df-from-schema"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con esquema explícito:\n",
      "+-----------+------+--------+\n",
      "|   producto|precio|cantidad|\n",
      "+-----------+------+--------+\n",
      "|     Laptop|1200.5|      10|\n",
      "| Smartphone|800.99|      20|\n",
      "|     Tablet|450.75|      15|\n",
      "|Auriculares|150.25|      30|\n",
      "+-----------+------+--------+\n",
      "\n",
      "root\n",
      " |-- producto: string (nullable = true)\n",
      " |-- precio: double (nullable = true)\n",
      " |-- cantidad: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Desde datos con esquema explícito\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Definir esquema\n",
    "schema = StructType([\n",
    "    StructField(\"producto\", StringType(), True),\n",
    "    StructField(\"precio\", DoubleType(), True),\n",
    "    StructField(\"cantidad\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Datos\n",
    "productos = [\n",
    "    (\"Laptop\", 1200.50, 10),\n",
    "    (\"Smartphone\", 800.99, 20),\n",
    "    (\"Tablet\", 450.75, 15),\n",
    "    (\"Auriculares\", 150.25, 30)\n",
    "]\n",
    "\n",
    "df3 = spark.createDataFrame(productos, schema)\n",
    "\n",
    "print(\"DataFrame con esquema explícito:\")\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "df-from-file"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'ciudades.csv': No such file or directory\n",
      "DataFrame desde CSV:\n",
      "+---+---------+--------+---------+\n",
      "| id|   ciudad|    pais|poblacion|\n",
      "+---+---------+--------+---------+\n",
      "|  1|   Madrid|  España|  3200000|\n",
      "|  2|Barcelona|  España|  1600000|\n",
      "|  3|   Lisboa|Portugal|   500000|\n",
      "|  4|    Paris| Francia|  2200000|\n",
      "+---+---------+--------+---------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      " |-- poblacion: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Desde archivos\n",
    "# Crear un CSV simple para el ejemplo\n",
    "# Borrar el archivo mal creado\n",
    "!rm ciudades.csv\n",
    "\n",
    "# Crear el CSV correctamente\n",
    "csv_content = \"\"\"id,ciudad,pais,poblacion\n",
    "1,Madrid,España,3200000\n",
    "2,Barcelona,España,1600000\n",
    "3,Lisboa,Portugal,500000\n",
    "4,Paris,Francia,2200000\"\"\"\n",
    "\n",
    "with open('ciudades.csv', 'w') as f:\n",
    "    f.write(csv_content)\n",
    "# Leer CSV\n",
    "df4 = spark.read.csv(\"ciudades.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"DataFrame desde CSV:\")\n",
    "df4.show()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df-operations"
   },
   "source": [
    "### Operaciones con DataFrames\n",
    "\n",
    "Los DataFrames ofrecen una API rica para manipular datos estructurados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "df-basic-ops"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selección de columnas:\n",
      "+-----------+------+\n",
      "|   producto|precio|\n",
      "+-----------+------+\n",
      "|     Laptop|1200.5|\n",
      "| Smartphone|800.99|\n",
      "|     Tablet|450.75|\n",
      "|Auriculares|150.25|\n",
      "+-----------+------+\n",
      "\n",
      "\n",
      "Filtrando productos con precio > 500:\n",
      "+----------+------+--------+\n",
      "|  producto|precio|cantidad|\n",
      "+----------+------+--------+\n",
      "|    Laptop|1200.5|      10|\n",
      "|Smartphone|800.99|      20|\n",
      "+----------+------+--------+\n",
      "\n",
      "\n",
      "Productos ordenados por precio (descendente):\n",
      "+-----------+------+--------+\n",
      "|   producto|precio|cantidad|\n",
      "+-----------+------+--------+\n",
      "|     Laptop|1200.5|      10|\n",
      "| Smartphone|800.99|      20|\n",
      "|     Tablet|450.75|      15|\n",
      "|Auriculares|150.25|      30|\n",
      "+-----------+------+--------+\n",
      "\n",
      "\n",
      "Añadiendo columna de valor total:\n",
      "+-----------+------+--------+-----------+\n",
      "|   producto|precio|cantidad|valor_total|\n",
      "+-----------+------+--------+-----------+\n",
      "|     Laptop|1200.5|      10|    12005.0|\n",
      "| Smartphone|800.99|      20|    16019.8|\n",
      "|     Tablet|450.75|      15|    6761.25|\n",
      "|Auriculares|150.25|      30|     4507.5|\n",
      "+-----------+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operaciones básicas con DataFrames\n",
    "\n",
    "# Seleccionar columnas\n",
    "print(\"Selección de columnas:\")\n",
    "df3.select(\"producto\", \"precio\").show()\n",
    "\n",
    "# Filtrar filas\n",
    "print(\"\\nFiltrando productos con precio > 500:\")\n",
    "df3.filter(df3.precio > 500).show()\n",
    "\n",
    "# Ordenar\n",
    "print(\"\\nProductos ordenados por precio (descendente):\")\n",
    "df3.orderBy(df3.precio.desc()).show()\n",
    "\n",
    "# Añadir columna calculada\n",
    "print(\"\\nAñadiendo columna de valor total:\")\n",
    "df3.withColumn(\"valor_total\", df3.precio * df3.cantidad).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "df-agg-ops"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadísticas descriptivas:\n",
      "+-------+-----------+------------------+-----------------+\n",
      "|summary|   producto|            precio|         cantidad|\n",
      "+-------+-----------+------------------+-----------------+\n",
      "|  count|          4|                 4|                4|\n",
      "|   mean|       NULL|          650.6225|            18.75|\n",
      "| stddev|       NULL|452.87868319414935|8.539125638299666|\n",
      "|    min|Auriculares|            150.25|               10|\n",
      "|    max|     Tablet|            1200.5|               30|\n",
      "+-------+-----------+------------------+-----------------+\n",
      "\n",
      "\n",
      "Agregaciones específicas:\n",
      "+--------------+---------------+-------------+-------------+\n",
      "|total_unidades|precio_promedio|precio_minimo|precio_maximo|\n",
      "+--------------+---------------+-------------+-------------+\n",
      "|            75|       650.6225|       150.25|       1200.5|\n",
      "+--------------+---------------+-------------+-------------+\n",
      "\n",
      "\n",
      "Agrupación por país y cálculo de población promedio:\n",
      "+--------+---------------+---------------+------------------+\n",
      "|    pais|numero_ciudades|poblacion_total|poblacion_promedio|\n",
      "+--------+---------------+---------------+------------------+\n",
      "| Francia|              1|        2200000|         2200000.0|\n",
      "|  España|              2|        4800000|         2400000.0|\n",
      "|Portugal|              1|         500000|          500000.0|\n",
      "+--------+---------------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operaciones de agregación\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"Estadísticas descriptivas:\")\n",
    "df3.describe().show()\n",
    "\n",
    "# Agregaciones específicas\n",
    "print(\"\\nAgregaciones específicas:\")\n",
    "from pyspark.sql.functions import sum, avg, min, max, count\n",
    "\n",
    "df3.select(\n",
    "    sum(\"cantidad\").alias(\"total_unidades\"),\n",
    "    avg(\"precio\").alias(\"precio_promedio\"),\n",
    "    min(\"precio\").alias(\"precio_minimo\"),\n",
    "    max(\"precio\").alias(\"precio_maximo\")\n",
    ").show()\n",
    "\n",
    "# Agrupar y agregar\n",
    "print(\"\\nAgrupación por país y cálculo de población promedio:\")\n",
    "df4.groupBy(\"pais\").agg(\n",
    "    count(\"ciudad\").alias(\"numero_ciudades\"),\n",
    "    sum(\"poblacion\").alias(\"poblacion_total\"),\n",
    "    avg(\"poblacion\").alias(\"poblacion_promedio\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "df-join-ops"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Clientes:\n",
      "+---+------------+------------------+\n",
      "| id|      nombre|             email|\n",
      "+---+------------+------------------+\n",
      "|  1|  Juan Pérez|  juan@example.com|\n",
      "|  2|María García| maria@example.com|\n",
      "|  3|Carlos López|carlos@example.com|\n",
      "|  4|Ana Martínez|   ana@example.com|\n",
      "+---+------------+------------------+\n",
      "\n",
      "\n",
      "DataFrame de Pedidos:\n",
      "+---------+----------+----------+-------+\n",
      "|id_pedido|id_cliente|     fecha|importe|\n",
      "+---------+----------+----------+-------+\n",
      "|      101|         1|2023-01-15|  120.5|\n",
      "|      102|         3|2023-01-18|  85.75|\n",
      "|      103|         2|2023-01-20|  220.0|\n",
      "|      104|         1|2023-01-25|   65.3|\n",
      "|      105|         5|2023-01-27| 110.25|\n",
      "+---------+----------+----------+-------+\n",
      "\n",
      "\n",
      "Inner Join:\n",
      "+---+------------+------------------+---------+----------+----------+-------+\n",
      "| id|      nombre|             email|id_pedido|id_cliente|     fecha|importe|\n",
      "+---+------------+------------------+---------+----------+----------+-------+\n",
      "|  1|  Juan Pérez|  juan@example.com|      101|         1|2023-01-15|  120.5|\n",
      "|  1|  Juan Pérez|  juan@example.com|      104|         1|2023-01-25|   65.3|\n",
      "|  2|María García| maria@example.com|      103|         2|2023-01-20|  220.0|\n",
      "|  3|Carlos López|carlos@example.com|      102|         3|2023-01-18|  85.75|\n",
      "+---+------------+------------------+---------+----------+----------+-------+\n",
      "\n",
      "\n",
      "Left Join:\n",
      "+---+------------+------------------+---------+----------+----------+-------+\n",
      "| id|      nombre|             email|id_pedido|id_cliente|     fecha|importe|\n",
      "+---+------------+------------------+---------+----------+----------+-------+\n",
      "|  1|  Juan Pérez|  juan@example.com|      104|         1|2023-01-25|   65.3|\n",
      "|  1|  Juan Pérez|  juan@example.com|      101|         1|2023-01-15|  120.5|\n",
      "|  2|María García| maria@example.com|      103|         2|2023-01-20|  220.0|\n",
      "|  3|Carlos López|carlos@example.com|      102|         3|2023-01-18|  85.75|\n",
      "|  4|Ana Martínez|   ana@example.com|     NULL|      NULL|      NULL|   NULL|\n",
      "+---+------------+------------------+---------+----------+----------+-------+\n",
      "\n",
      "\n",
      "Right Join:\n",
      "+----+------------+------------------+---------+----------+----------+-------+\n",
      "|  id|      nombre|             email|id_pedido|id_cliente|     fecha|importe|\n",
      "+----+------------+------------------+---------+----------+----------+-------+\n",
      "|   1|  Juan Pérez|  juan@example.com|      101|         1|2023-01-15|  120.5|\n",
      "|   3|Carlos López|carlos@example.com|      102|         3|2023-01-18|  85.75|\n",
      "|   2|María García| maria@example.com|      103|         2|2023-01-20|  220.0|\n",
      "|   1|  Juan Pérez|  juan@example.com|      104|         1|2023-01-25|   65.3|\n",
      "|NULL|        NULL|              NULL|      105|         5|2023-01-27| 110.25|\n",
      "+----+------------+------------------+---------+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operaciones de join\n",
    "\n",
    "# Crear DataFrames para demostrar joins\n",
    "clientes = spark.createDataFrame([\n",
    "    (1, \"Juan Pérez\", \"juan@example.com\"),\n",
    "    (2, \"María García\", \"maria@example.com\"),\n",
    "    (3, \"Carlos López\", \"carlos@example.com\"),\n",
    "    (4, \"Ana Martínez\", \"ana@example.com\")\n",
    "], [\"id\", \"nombre\", \"email\"])\n",
    "\n",
    "pedidos = spark.createDataFrame([\n",
    "    (101, 1, \"2023-01-15\", 120.50),\n",
    "    (102, 3, \"2023-01-18\", 85.75),\n",
    "    (103, 2, \"2023-01-20\", 220.00),\n",
    "    (104, 1, \"2023-01-25\", 65.30),\n",
    "    (105, 5, \"2023-01-27\", 110.25)  # Cliente 5 no existe\n",
    "], [\"id_pedido\", \"id_cliente\", \"fecha\", \"importe\"])\n",
    "\n",
    "print(\"DataFrame de Clientes:\")\n",
    "clientes.show()\n",
    "\n",
    "print(\"\\nDataFrame de Pedidos:\")\n",
    "pedidos.show()\n",
    "\n",
    "# Inner join\n",
    "print(\"\\nInner Join:\")\n",
    "clientes.join(pedidos, clientes.id == pedidos.id_cliente).show()\n",
    "\n",
    "# Left join\n",
    "print(\"\\nLeft Join:\")\n",
    "clientes.join(pedidos, clientes.id == pedidos.id_cliente, \"left\").show()\n",
    "\n",
    "# Right join\n",
    "print(\"\\nRight Join:\")\n",
    "clientes.join(pedidos, clientes.id == pedidos.id_cliente, \"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df-udf"
   },
   "source": [
    "### Funciones definidas por el usuario (UDFs)\n",
    "\n",
    "Las UDFs permiten aplicar funciones personalizadas a los datos en DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "udf-examples"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres en mayúsculas:\n",
      "+---+------------+------------------+------------+\n",
      "| id|      nombre|             email|nombre_mayus|\n",
      "+---+------------+------------------+------------+\n",
      "|  1|  Juan Pérez|  juan@example.com|  JUAN PÉREZ|\n",
      "|  2|María García| maria@example.com|MARÍA GARCÍA|\n",
      "|  3|Carlos López|carlos@example.com|CARLOS LÓPEZ|\n",
      "|  4|Ana Martínez|   ana@example.com|ANA MARTÍNEZ|\n",
      "+---+------------+------------------+------------+\n",
      "\n",
      "\n",
      "Productos clasificados por precio:\n",
      "+-----------+------+--------+---------+\n",
      "|   producto|precio|cantidad|categoria|\n",
      "+-----------+------+--------+---------+\n",
      "|     Laptop|1200.5|      10|  Premium|\n",
      "| Smartphone|800.99|      20|  Premium|\n",
      "|     Tablet|450.75|      15|    Medio|\n",
      "|Auriculares|150.25|      30|Económico|\n",
      "+-----------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# UDF para convertir nombres a mayúsculas\n",
    "upper_udf = udf(lambda x: x.upper(), StringType())\n",
    "\n",
    "print(\"Nombres en mayúsculas:\")\n",
    "clientes.withColumn(\"nombre_mayus\", upper_udf(clientes.nombre)).show()\n",
    "\n",
    "# UDF para clasificar productos según su precio\n",
    "def clasificar_precio(precio):\n",
    "    if precio < 200:\n",
    "        return \"Económico\"\n",
    "    elif precio < 800:\n",
    "        return \"Medio\"\n",
    "    else:\n",
    "        return \"Premium\"\n",
    "\n",
    "clasificar_udf = udf(clasificar_precio, StringType())\n",
    "\n",
    "print(\"\\nProductos clasificados por precio:\")\n",
    "df3.withColumn(\"categoria\", clasificar_udf(df3.precio)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spark-sql-section"
   },
   "source": [
    "## 5. Spark SQL\n",
    "\n",
    "Spark SQL permite ejecutar consultas SQL sobre datos estructurados. Es una capa encima de la API de DataFrames que proporciona una interfaz para programación con SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "temp-views"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productos con precio > 500:\n",
      "+----------+------+--------+-----------+\n",
      "|  producto|precio|cantidad|valor_total|\n",
      "+----------+------+--------+-----------+\n",
      "|    Laptop|1200.5|      10|    12005.0|\n",
      "|Smartphone|800.99|      20|    16019.8|\n",
      "+----------+------+--------+-----------+\n",
      "\n",
      "\n",
      "Grupos de ciudades por país:\n",
      "+--------+------------+---------------+------------------+\n",
      "|    pais|num_ciudades|poblacion_total|poblacion_promedio|\n",
      "+--------+------------+---------------+------------------+\n",
      "| Francia|           1|        2200000|         2200000.0|\n",
      "|  España|           2|        4800000|         2400000.0|\n",
      "|Portugal|           1|         500000|          500000.0|\n",
      "+--------+------------+---------------+------------------+\n",
      "\n",
      "\n",
      "Clientes con sus pedidos:\n",
      "+---+------------+---------+----------+-------+\n",
      "| id|      nombre|id_pedido|     fecha|importe|\n",
      "+---+------------+---------+----------+-------+\n",
      "|  1|  Juan Pérez|      101|2023-01-15|  120.5|\n",
      "|  1|  Juan Pérez|      104|2023-01-25|   65.3|\n",
      "|  2|María García|      103|2023-01-20|  220.0|\n",
      "|  3|Carlos López|      102|2023-01-18|  85.75|\n",
      "|  4|Ana Martínez|     NULL|      NULL|   NULL|\n",
      "+---+------------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrar DataFrames como vistas temporales\n",
    "df3.createOrReplaceTempView(\"productos\")\n",
    "df4.createOrReplaceTempView(\"ciudades\")\n",
    "clientes.createOrReplaceTempView(\"clientes\")\n",
    "pedidos.createOrReplaceTempView(\"pedidos\")\n",
    "\n",
    "# Ejecutar consultas SQL\n",
    "print(\"Productos con precio > 500:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    producto, \n",
    "    precio, \n",
    "    cantidad, \n",
    "    precio * cantidad AS valor_total\n",
    "FROM productos\n",
    "WHERE precio > 500\n",
    "ORDER BY precio DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nGrupos de ciudades por país:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pais, \n",
    "    COUNT(*) AS num_ciudades, \n",
    "    SUM(poblacion) AS poblacion_total, \n",
    "    AVG(poblacion) AS poblacion_promedio\n",
    "FROM ciudades\n",
    "GROUP BY pais\n",
    "HAVING COUNT(*) > 0\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nClientes con sus pedidos:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.id, \n",
    "    c.nombre, \n",
    "    p.id_pedido, \n",
    "    p.fecha, \n",
    "    p.importe\n",
    "FROM clientes c\n",
    "LEFT JOIN pedidos p ON c.id = p.id_cliente\n",
    "ORDER BY c.id, p.fecha\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sql-window-functions"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de pedidos por cliente con Window Functions:\n",
      "+---+------------+---------+----------+-------+-------------+---------------+\n",
      "| id|      nombre|id_pedido|     fecha|importe|total_cliente|ranking_importe|\n",
      "+---+------------+---------+----------+-------+-------------+---------------+\n",
      "|  1|  Juan Pérez|      101|2023-01-15|  120.5|        185.8|              1|\n",
      "|  1|  Juan Pérez|      104|2023-01-25|   65.3|        185.8|              2|\n",
      "|  2|María García|      103|2023-01-20|  220.0|        220.0|              1|\n",
      "|  3|Carlos López|      102|2023-01-18|  85.75|        85.75|              1|\n",
      "+---+------------+---------+----------+-------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de funciones de ventana con SQL\n",
    "print(\"Análisis de pedidos por cliente con Window Functions:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.id, \n",
    "    c.nombre, \n",
    "    p.id_pedido, \n",
    "    p.fecha, \n",
    "    p.importe,\n",
    "    SUM(p.importe) OVER (PARTITION BY c.id) AS total_cliente,\n",
    "    RANK() OVER (PARTITION BY c.id ORDER BY p.importe DESC) AS ranking_importe\n",
    "FROM clientes c\n",
    "JOIN pedidos p ON c.id = p.id_cliente\n",
    "ORDER BY c.id, ranking_importe\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimization"
   },
   "source": [
    "### Optimización de consultas\n",
    "\n",
    "Spark SQL utiliza el optimizador Catalyst para transformar consultas y mejorar su rendimiento. Podemos ver el plan de ejecución con el método `explain()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "explain-plan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+---------------+\n",
      "|  pais|num_productos|valor_total|precio_promedio|\n",
      "+------+-------------+-----------+---------------+\n",
      "|España|            4|   39293.55|       650.6225|\n",
      "+------+-------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, countDistinct, sum, avg, monotonically_increasing_id\n",
    "\n",
    "# Opción 2: Asociar productos a ciudades (con relación artificial)\n",
    "# Agregar ID temporal a productos\n",
    "productos_con_id = df3.withColumn(\"temp_id\", monotonically_increasing_id())\n",
    "\n",
    "# Crear relación producto-ciudad (ejemplo: rotación modular)\n",
    "relacion_producto_ciudad = productos_con_id.withColumn(\n",
    "    \"ciudad_id\",\n",
    "    (col(\"temp_id\") % 4) + 1  # Asigna a ciudades con ID 1-4\n",
    ")\n",
    "\n",
    "# Hacer join con ciudades\n",
    "analisis_por_pais = relacion_producto_ciudad.join(\n",
    "    df4, \n",
    "    relacion_producto_ciudad.ciudad_id == df4.id\n",
    ").groupBy(\"pais\").agg(\n",
    "    countDistinct(\"producto\").alias(\"num_productos\"),\n",
    "    sum(col(\"precio\") * col(\"cantidad\")).alias(\"valor_total\"),\n",
    "    avg(\"precio\").alias(\"precio_promedio\")\n",
    ").orderBy(\"valor_total\", ascending=False)\n",
    "\n",
    "analisis_por_pais.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise-section"
   },
   "source": [
    "## 6. Ejercicio práctico: Análisis de datos reales\n",
    "\n",
    "Vamos a trabajar con un conjunto de datos real: el dataset de E-Commerce. Primero, descarguemos el dataset desde Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.11/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer in /opt/conda/lib/python3.11/site-packages (from kaggle) (3.3.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from kaggle) (3.4)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.8.2)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from kaggle) (68.2.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kaggle) (4.66.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from kaggle) (2.0.7)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from kaggle) (0.5.1)\n",
      "Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.7.4.2 python-slugify-8.0.4 text-unidecode-1.3\n",
      "Dataset de ejemplo creado con éxito!\n",
      "   InvoiceNo  StockCode Description  Quantity InvoiceDate   UnitPrice  \\\n",
      "0          1      25795      Tablet         4  2022-06-05  487.355480   \n",
      "1          2      10860     Monitor         3  2022-06-21  273.029337   \n",
      "2          3      86820     Monitor         8  2022-12-01  307.649194   \n",
      "3          4      64886     Monitor         4  2022-02-10  293.136811   \n",
      "4          5      16265      Tablet         2  2022-03-04  543.626714   \n",
      "\n",
      "   CustomerID      Country     Category  \n",
      "0       12350       México  Electrónica  \n",
      "1       11269      Francia  Informática  \n",
      "2       11260  Reino Unido  Informática  \n",
      "3       17701      Francia  Informática  \n",
      "4       18376     Colombia  Electrónica  \n"
     ]
    }
   ],
   "source": [
    "# Instalamos la API de Kaggle y configuramos credenciales\n",
    "!pip install kaggle\n",
    "\n",
    "# Nota: Para usar la API de Kaggle, necesitas tus credenciales\n",
    "# Puedes descargar kaggle.json desde tu cuenta de Kaggle y subir manualmente\n",
    "# O usar un dataset de ejemplo que creamos aquí\n",
    "\n",
    "# Creamos un dataset de ejemplo (simulando datos de E-Commerce)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generar datos aleatorios\n",
    "np.random.seed(42)\n",
    "n_rows = 1000\n",
    "\n",
    "# Productos\n",
    "productos = ['Laptop', 'Smartphone', 'Tablet', 'Auriculares', 'Monitor', 'Teclado', 'Mouse', 'Impresora', 'Cámara', 'Altavoces']\n",
    "categorias = ['Electrónica', 'Informática', 'Accesorios', 'Audio', 'Fotografía']\n",
    "product_category = {\n",
    "    'Laptop': 'Informática',\n",
    "    'Smartphone': 'Electrónica',\n",
    "    'Tablet': 'Electrónica',\n",
    "    'Auriculares': 'Audio',\n",
    "    'Monitor': 'Informática',\n",
    "    'Teclado': 'Accesorios',\n",
    "    'Mouse': 'Accesorios',\n",
    "    'Impresora': 'Informática',\n",
    "    'Cámara': 'Fotografía',\n",
    "    'Altavoces': 'Audio'\n",
    "}\n",
    "\n",
    "# Precios base\n",
    "precios_base = {\n",
    "    'Laptop': 1200,\n",
    "    'Smartphone': 800,\n",
    "    'Tablet': 500,\n",
    "    'Auriculares': 150,\n",
    "    'Monitor': 300,\n",
    "    'Teclado': 80,\n",
    "    'Mouse': 50,\n",
    "    'Impresora': 250,\n",
    "    'Cámara': 600,\n",
    "    'Altavoces': 200\n",
    "}\n",
    "\n",
    "# Países\n",
    "paises = ['España', 'México', 'Argentina', 'Colombia', 'Chile', 'Perú', 'Estados Unidos', 'Reino Unido', 'Francia', 'Alemania']\n",
    "\n",
    "# Generar datos\n",
    "invoice_no = np.arange(1, n_rows+1)\n",
    "stock_code = np.random.randint(10000, 99999, size=n_rows)\n",
    "description = np.random.choice(productos, size=n_rows)\n",
    "quantity = np.random.randint(1, 10, size=n_rows)\n",
    "\n",
    "# Fechas entre 2022-01-01 y 2022-12-31 (parte corregida)\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2022, 12, 31)\n",
    "days_between = (end_date - start_date).days\n",
    "random_days = np.random.randint(0, days_between, size=n_rows)\n",
    "invoice_date = [start_date + timedelta(days=int(day)) for day in random_days]  # Conversión a int\n",
    "\n",
    "# Precios con variación aleatoria\n",
    "unit_price = [precios_base[prod] * (0.9 + np.random.random() * 0.2) for prod in description]\n",
    "\n",
    "# Clientes (500 clientes únicos)\n",
    "customer_id = np.random.randint(10000, 19999, size=n_rows)\n",
    "country = np.random.choice(paises, size=n_rows)\n",
    "\n",
    "# Categoría\n",
    "category = [product_category[prod] for prod in description]\n",
    "\n",
    "# Crear DataFrame\n",
    "ecommerce_df = pd.DataFrame({\n",
    "    'InvoiceNo': invoice_no,\n",
    "    'StockCode': stock_code,\n",
    "    'Description': description,\n",
    "    'Quantity': quantity,\n",
    "    'InvoiceDate': invoice_date,\n",
    "    'UnitPrice': unit_price,\n",
    "    'CustomerID': customer_id,\n",
    "    'Country': country,\n",
    "    'Category': category\n",
    "})\n",
    "\n",
    "# Guardar como CSV\n",
    "ecommerce_df.to_csv('../data/ecommerce_data.csv', index=False)\n",
    "print(\"Dataset de ejemplo creado con éxito!\")\n",
    "print(ecommerce_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "load-ecommerce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: integer (nullable = true)\n",
      " |-- StockCode: integer (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: date (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n",
      "+---------+---------+-----------+--------+-----------+-----------------+----------+-----------+-----------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|        UnitPrice|CustomerID|    Country|   Category|\n",
      "+---------+---------+-----------+--------+-----------+-----------------+----------+-----------+-----------+\n",
      "|        1|    25795|     Tablet|       4| 2022-06-05|487.3554801047682|     12350|     México|Electrónica|\n",
      "|        2|    10860|    Monitor|       3| 2022-06-21|273.0293366647445|     11269|    Francia|Informática|\n",
      "|        3|    86820|    Monitor|       8| 2022-12-01|307.6491943070568|     11260|Reino Unido|Informática|\n",
      "|        4|    64886|    Monitor|       4| 2022-02-10|293.1368107719389|     17701|    Francia|Informática|\n",
      "|        5|    16265|     Tablet|       2| 2022-03-04|543.6267139413219|     18376|   Colombia|Electrónica|\n",
      "+---------+---------+-----------+--------+-----------+-----------------+----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset en Spark\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, to_date, expr\n",
    "\n",
    "# Definir esquema para mejor rendimiento\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Cargar datos CSV\n",
    "sales_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"../data/ecommerce_data.csv\")\n",
    "\n",
    "# Mostrar esquema y primeras filas\n",
    "sales_df.printSchema()\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "data-preprocessing"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+-----------------+----------+-----------+-----------+------------------+-----------+------------+----------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|        UnitPrice|CustomerID|    Country|   Category|       TotalAmount|InvoiceYear|InvoiceMonth|InvoiceDay|\n",
      "+---------+---------+-----------+--------+-----------+-----------------+----------+-----------+-----------+------------------+-----------+------------+----------+\n",
      "|        1|    25795|     Tablet|       4| 2022-06-05|487.3554801047682|     12350|     México|Electrónica|1949.4219204190729|       2022|           6|         5|\n",
      "|        2|    10860|    Monitor|       3| 2022-06-21|273.0293366647445|     11269|    Francia|Informática| 819.0880099942335|       2022|           6|        21|\n",
      "|        3|    86820|    Monitor|       8| 2022-12-01|307.6491943070568|     11260|Reino Unido|Informática|2461.1935544564544|       2022|          12|         1|\n",
      "|        4|    64886|    Monitor|       4| 2022-02-10|293.1368107719389|     17701|    Francia|Informática|1172.5472430877555|       2022|           2|        10|\n",
      "|        5|    16265|     Tablet|       2| 2022-03-04|543.6267139413219|     18376|   Colombia|Electrónica|1087.2534278826438|       2022|           3|         4|\n",
      "+---------+---------+-----------+--------+-----------+-----------------+----------+-----------+-----------+------------------+-----------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "\n",
    "# Añadir columna de importe total\n",
    "sales_df = sales_df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
    "\n",
    "# Filtrar registros válidos (VERSIÓN CORREGIDA)\n",
    "valid_sales = sales_df.filter(\n",
    "    (col(\"Quantity\") > 0) &  # Cada condición entre paréntesis\n",
    "    (col(\"UnitPrice\") > 0) & \n",
    "    (col(\"CustomerID\").isNotNull())\n",
    ")\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "sales_with_date = valid_sales \\\n",
    "    .withColumn(\"InvoiceYear\", year(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"InvoiceMonth\", month(col(\"InvoiceDate\"))) \\\n",
    "    .withColumn(\"InvoiceDay\", dayofmonth(col(\"InvoiceDate\")))\n",
    "\n",
    "# Guardar como vista temporal para SQL\n",
    "sales_with_date.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Mostrar el resultado del preprocesamiento\n",
    "sales_with_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sales-analysis"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventas por país:\n",
      "+--------------+------------------+--------------------+---------------+-----------------------+\n",
      "|       Country|        TotalSales|NumberOfTransactions|UniqueCustomers|AverageTransactionValue|\n",
      "+--------------+------------------+--------------------+---------------+-----------------------+\n",
      "|      Alemania| 269689.3437270837|                 123|            122|      2192.596290464095|\n",
      "|          Perú|255152.10721917168|                 101|             99|     2526.2584873185315|\n",
      "|Estados Unidos|249191.25035328942|                 103|            102|      2419.332527701839|\n",
      "|        México|247759.50095566988|                 105|            104|     2359.6142948159036|\n",
      "|         Chile| 207585.8689953313|                 109|            108|     1904.4575137186357|\n",
      "|      Colombia| 206236.5155897261|                 101|            100|      2041.945698908179|\n",
      "|        España| 184932.9844480791|                  93|             91|     1988.5267144954744|\n",
      "|   Reino Unido| 181123.3864928343|                  94|             94|     1926.8445371578118|\n",
      "|       Francia|169612.29142539806|                  86|             85|     1972.2359468069542|\n",
      "|     Argentina|154704.23250644747|                  85|             85|     1820.0497941934996|\n",
      "+--------------+------------------+--------------------+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análisis 1: Ventas por país\n",
    "country_sales = valid_sales \\\n",
    "    .groupBy(\"Country\") \\\n",
    "    .agg(\n",
    "        sum(\"TotalAmount\").alias(\"TotalSales\"),\n",
    "        count(\"InvoiceNo\").alias(\"NumberOfTransactions\"),\n",
    "        countDistinct(\"CustomerID\").alias(\"UniqueCustomers\"),\n",
    "        avg(\"TotalAmount\").alias(\"AverageTransactionValue\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"TotalSales\").desc())\n",
    "\n",
    "print(\"Ventas por país:\")\n",
    "country_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "monthly-sales"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventas mensuales:\n",
      "+-----------+------------+------------------+\n",
      "|InvoiceYear|InvoiceMonth|      MonthlySales|\n",
      "+-----------+------------+------------------+\n",
      "|       2022|           1| 218126.2293537872|\n",
      "|       2022|           2| 157076.3029914686|\n",
      "|       2022|           3| 144535.6833339949|\n",
      "|       2022|           4| 231195.9429719927|\n",
      "|       2022|           5|166185.54915347442|\n",
      "|       2022|           6| 213549.5577464724|\n",
      "|       2022|           7| 188387.0938485728|\n",
      "|       2022|           8|141107.81360383847|\n",
      "|       2022|           9|   138842.63058643|\n",
      "|       2022|          10|157190.78718466626|\n",
      "|       2022|          11| 189568.6890119039|\n",
      "|       2022|          12|180221.20192642946|\n",
      "+-----------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análisis 2: Ventas por mes\n",
    "monthly_sales = sales_with_date \\\n",
    "    .groupBy(\"InvoiceYear\", \"InvoiceMonth\") \\\n",
    "    .agg(sum(\"TotalAmount\").alias(\"MonthlySales\")) \\\n",
    "    .orderBy(\"InvoiceYear\", \"InvoiceMonth\")\n",
    "\n",
    "print(\"Ventas mensuales:\")\n",
    "monthly_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "products-analysis"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top productos más vendidos:\n",
      "+-----------+-----------+-------------+------------------+------------+\n",
      "|Description|   Category|TotalQuantity|      TotalRevenue|TimesOrdered|\n",
      "+-----------+-----------+-------------+------------------+------------+\n",
      "|  Impresora|Informática|          587|147819.45644712358|         104|\n",
      "| Smartphone|Electrónica|          556|446559.81206883455|         110|\n",
      "|    Monitor|Informática|          538|159613.39130268156|         103|\n",
      "|     Cámara| Fotografía|          514|305325.25830868346|         100|\n",
      "|     Laptop|Informática|          511| 617639.7801776406|         106|\n",
      "|    Teclado| Accesorios|          507| 40690.36405932423|          98|\n",
      "|      Mouse| Accesorios|          501|  25065.4345990297|          97|\n",
      "|     Tablet|Electrónica|          458|230217.90379250183|          97|\n",
      "|  Altavoces|      Audio|          456|  91458.7916752584|          96|\n",
      "|Auriculares|      Audio|          412| 61597.28928195333|          89|\n",
      "+-----------+-----------+-------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análisis 3: Productos más vendidos\n",
    "top_products = valid_sales \\\n",
    "    .groupBy(\"Description\", \"Category\") \\\n",
    "    .agg(\n",
    "        sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "        sum(\"TotalAmount\").alias(\"TotalRevenue\"),\n",
    "        count(\"InvoiceNo\").alias(\"TimesOrdered\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"TotalQuantity\").desc())\n",
    "\n",
    "print(\"Top productos más vendidos:\")\n",
    "top_products.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "category-analysis"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventas por categoría:\n",
      "+-----------+------------------+-------------+--------------+\n",
      "|   Category|        TotalSales|TotalQuantity|UniqueProducts|\n",
      "+-----------+------------------+-------------+--------------+\n",
      "|Informática| 925072.6279274457|         1636|             3|\n",
      "|Electrónica| 676777.7158613363|         1014|             2|\n",
      "| Fotografía|305325.25830868346|          514|             1|\n",
      "|      Audio|153056.08095721173|          868|             2|\n",
      "| Accesorios| 65755.79865835393|         1008|             2|\n",
      "+-----------+------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análisis 4: Ventas por categoría de producto\n",
    "category_sales = valid_sales \\\n",
    "    .groupBy(\"Category\") \\\n",
    "    .agg(\n",
    "        sum(\"TotalAmount\").alias(\"TotalSales\"),\n",
    "        sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
    "        countDistinct(\"Description\").alias(\"UniqueProducts\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"TotalSales\").desc())\n",
    "\n",
    "print(\"Ventas por categoría:\")\n",
    "category_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "customer-rfm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentación de clientes (RFM):\n",
      "+-------------------+-------------+------------------+\n",
      "|            Segment|CustomerCount|      AverageSpend|\n",
      "+-------------------+-------------+------------------+\n",
      "|    Loyal Customers|           42|  5837.11226771252|\n",
      "|Potential Loyalists|          263|3692.9203136183496|\n",
      "|            At Risk|          507|1718.0587056503234|\n",
      "|        Hibernating|          135| 285.4441497982637|\n",
      "+-------------------+-------------+------------------+\n",
      "\n",
      "\n",
      "Ejemplos de clientes por segmento:\n",
      "+----------+-------+---------+------------------+------------+--------------+-------------+--------+---------------+\n",
      "|CustomerID|Recency|Frequency|     MonetaryValue|RecencyScore|FrequencyScore|MonetaryScore|RFMScore|        Segment|\n",
      "+----------+-------+---------+------------------+------------+--------------+-------------+--------+---------------+\n",
      "|     11301|      6|        2| 9125.115667016027|           5|             2|            5|      12|Loyal Customers|\n",
      "|     10178|     36|        2| 7547.057914073749|           4|             2|            5|      11|Loyal Customers|\n",
      "|     18386|      1|        1|5169.5657358533645|           5|             1|            5|      11|Loyal Customers|\n",
      "|     11191|     24|        2| 2867.009919160844|           5|             2|            4|      11|Loyal Customers|\n",
      "|     15077|     12|        1|10400.528732545758|           5|             1|            5|      11|Loyal Customers|\n",
      "+----------+-------+---------+------------------+------------+--------------+-------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análisis 5: RFM Analysis (Recency, Frequency, Monetary)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import datediff, max as sql_max, lit, when, col, countDistinct, sum, count, avg\n",
    "from pyspark.sql.functions import desc  # <-- Esta es la importación que faltaba\n",
    "\n",
    "# Obtener fecha máxima en el dataset\n",
    "max_date = valid_sales.agg(sql_max(\"InvoiceDate\")).collect()[0][0]\n",
    "\n",
    "# Calcular métricas RFM\n",
    "rfm = valid_sales \\\n",
    "    .groupBy(\"CustomerID\") \\\n",
    "    .agg(\n",
    "        datediff(lit(max_date), sql_max(\"InvoiceDate\")).alias(\"Recency\"),\n",
    "        countDistinct(\"InvoiceNo\").alias(\"Frequency\"),\n",
    "        sum(\"TotalAmount\").alias(\"MonetaryValue\")\n",
    "    )\n",
    "\n",
    "# Categorizar clientes por segmentos\n",
    "rfm = rfm \\\n",
    "    .withColumn(\"RecencyScore\", \n",
    "                when(col(\"Recency\") <= 30, 5)\n",
    "                .when(col(\"Recency\") <= 60, 4)\n",
    "                .when(col(\"Recency\") <= 90, 3)\n",
    "                .when(col(\"Recency\") <= 120, 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"FrequencyScore\",\n",
    "                when(col(\"Frequency\") >= 20, 5)\n",
    "                .when(col(\"Frequency\") >= 10, 4)\n",
    "                .when(col(\"Frequency\") >= 5, 3)\n",
    "                .when(col(\"Frequency\") >= 2, 2)\n",
    "                .otherwise(1)) \\\n",
    "    .withColumn(\"MonetaryScore\",\n",
    "                when(col(\"MonetaryValue\") >= 5000, 5)\n",
    "                .when(col(\"MonetaryValue\") >= 2500, 4)\n",
    "                .when(col(\"MonetaryValue\") >= 1000, 3)\n",
    "                .when(col(\"MonetaryValue\") >= 500, 2)\n",
    "                .otherwise(1))\n",
    "\n",
    "# Calcular RFM Score final\n",
    "rfm = rfm.withColumn(\"RFMScore\", \n",
    "                     col(\"RecencyScore\") + col(\"FrequencyScore\") + col(\"MonetaryScore\"))\n",
    "\n",
    "# Mostrar resultados de clientes por segmento\n",
    "rfm_segments = rfm \\\n",
    "    .withColumn(\"Segment\", \n",
    "                when(col(\"RFMScore\") >= 13, \"Champions\")\n",
    "                .when(col(\"RFMScore\") >= 10, \"Loyal Customers\")\n",
    "                .when(col(\"RFMScore\") >= 7, \"Potential Loyalists\")\n",
    "                .when(col(\"RFMScore\") >= 4, \"At Risk\")\n",
    "                .otherwise(\"Hibernating\"))\n",
    "\n",
    "segment_summary = rfm_segments \\\n",
    "    .groupBy(\"Segment\") \\\n",
    "    .agg(\n",
    "        count(\"CustomerID\").alias(\"CustomerCount\"),\n",
    "        avg(\"MonetaryValue\").alias(\"AverageSpend\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"AverageSpend\"))  # <-- Ahora funcionará correctamente\n",
    "\n",
    "print(\"Segmentación de clientes (RFM):\")\n",
    "segment_summary.show()\n",
    "\n",
    "# Mostrar algunos ejemplos de clientes por segmento\n",
    "print(\"\\nEjemplos de clientes por segmento:\")\n",
    "rfm_segments.orderBy(desc(\"RFMScore\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "save-output"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados!\n"
     ]
    }
   ],
   "source": [
    "# Guardar resultados procesados\n",
    "country_sales.write.mode(\"overwrite\").csv(\"country_sales.csv\")\n",
    "top_products.write.mode(\"overwrite\").csv(\"top_products.csv\")\n",
    "rfm_segments.write.mode(\"overwrite\").csv(\"customer_segments.csv\")\n",
    "\n",
    "print(\"Resultados guardados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Conclusión\n",
    "\n",
    "En este notebook, hemos aprendido los fundamentos de Apache Spark, trabajando con RDDs, DataFrames y Spark SQL. También hemos aplicado estos conocimientos a un análisis de datos real de E-Commerce.\n",
    "\n",
    "Conceptos clave que hemos cubierto:\n",
    "1. RDDs: operaciones básicas, transformaciones, acciones y persistencia\n",
    "2. DataFrames: creación, operaciones y agregaciones\n",
    "3. Spark SQL: consultas SQL en datos estructurados\n",
    "4. Análisis de datos reales con técnicas como RFM\n",
    "\n",
    "Próximos pasos para seguir aprendiendo:\n",
    "- Explorar Spark Streaming para datos en tiempo real\n",
    "- Aprender MLlib para machine learning distribuido\n",
    "- Profundizar en GraphX para procesamiento de grafos\n",
    "- Trabajar con clusters de Spark reales\n",
    "\n",
    "Para más información, consulta la [documentación oficial de Apache Spark](https://spark.apache.org/docs/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesión de Spark detenida.\n"
     ]
    }
   ],
   "source": [
    "# Detener la sesión de Spark\n",
    "spark.stop()\n",
    "print(\"Sesión de Spark detenida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
